{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Captal Letters Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-23T10:53:49.112590Z",
     "start_time": "2018-03-23T10:53:45.468611Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package semcor to /Users/aloosley/nltk_data...\n",
      "[nltk_data]   Package semcor is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/aloosley/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package perluniprops to\n",
      "[nltk_data]     /Users/aloosley/nltk_data...\n",
      "[nltk_data]   Package perluniprops is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aloosley/anaconda2/envs/py3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import nltk\n",
    "nltk.download('semcor')\n",
    "nltk.download('punkt')\n",
    "nltk.download('perluniprops')\n",
    "from nltk.corpus import semcor\n",
    "from nltk.tokenize.moses import MosesDetokenizer\n",
    "from keras.layers import Embedding, LSTM, GRU, Conv1D, Dense, Bidirectional\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import numpy as np\n",
    "\n",
    "# detokenization: turnig tokens back into sentences\n",
    "MDETOK = MosesDetokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Brown / Semcor Corpus, select sentences with lots of capitalized words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-23T10:53:52.655397Z",
     "start_time": "2018-03-23T10:53:52.645263Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_titled_sents(sentences, u_case_min_nb=3):\n",
    "    '''select only those sentences with at least u_case_min_nb number of words beginning with capital letter'''\n",
    "    filtered_sents = []\n",
    "    for sent in sentences:\n",
    "        nb_titles = 0\n",
    "        for token in sent:\n",
    "            if token.istitle():\n",
    "                nb_titles += 1\n",
    "        if nb_titles >= u_case_min_nb:\n",
    "            filtered_sents.append(sent)\n",
    "    return filtered_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-23T10:54:29.614493Z",
     "start_time": "2018-03-23T10:54:00.820332Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of sentences: 37176\n",
      "number of sentences after filtering: 10031\n",
      "sample sentence:\n",
      "['Albert', 'Einstein', 'was', 'quoted', 'as', 'saying', ':', '``', 'The', 'workings', 'of', 'the', 'woman', \"'s\", 'mind', 'amaze', 'me', \"''\", '.']\n"
     ]
    }
   ],
   "source": [
    "sents = semcor.sents()  # loading tokenized sentences from Semcor corpus\n",
    "print(\"number of sentences: %s\" % len(sents))\n",
    "sents = filter_titled_sents(sents, u_case_min_nb=3)\n",
    "print(\"number of sentences after filtering: %s\" % len(sents))\n",
    "print(\"sample sentence:\")\n",
    "print(sents[10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-23T10:54:52.741825Z",
     "start_time": "2018-03-23T10:54:52.589711Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of sentences after filtering: 7144\n"
     ]
    }
   ],
   "source": [
    "# we are going to keep only shorter sentences\n",
    "MAX_SEQUENCE_LENGTH = 35\n",
    "sents = [sent for sent in sents if len(sent) <= MAX_SEQUENCE_LENGTH]\n",
    "print(\"number of sentences after filtering: %s\" % len(sents))\n",
    "\n",
    "# and clean the dataset a bit removing tokens like `` first\n",
    "filter_out_toks = {'``', '\\'\\''}\n",
    "sents =[[word for word in sent if word not in filter_out_toks] for sent in sents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Word Level Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task we are going to use pre-trained \"GloVe\" word embeddings that can be downloaded from https://nlp.stanford.edu/data/glove.6B.zip\n",
    "\n",
    "You can have a look at the visualization of pre-trained Word2Vec word embeddings, words with similar meaning are clustered together: http://projector.tensorflow.org/\n",
    "\n",
    "They used PCA or T-SNE to reduce the dimensionality of the vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-23T10:55:03.271285Z",
     "start_time": "2018-03-23T10:55:03.167927Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens anotated based on their first letter case:\n",
      "<zip object at 0x11086b488>\n"
     ]
    }
   ],
   "source": [
    "# make sentences lowercase\n",
    "sents_lower =[[word.lower() for word in sent] for sent in sents]\n",
    "# annotate words in sentences based on their first letter case\n",
    "capitalization_sent_tags = [[word.istitle() for word in sent] for sent in sents]\n",
    "print(\"tokens anotated based on their first letter case:\")\n",
    "print(zip(sents_lower[-500], capitalization_sent_tags[-500]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-23T10:55:03.879524Z",
     "start_time": "2018-03-23T10:55:03.845389Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size: 18482\n"
     ]
    }
   ],
   "source": [
    "# create a vocabulary of all words in our dataset\n",
    "words = set([])\n",
    "for sent in sents_lower:\n",
    "    words.update(sent)\n",
    "print(\"vocabulary size: %s\" % len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-23T10:55:05.001718Z",
     "start_time": "2018-03-23T10:55:04.974168Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index of `hello`: 12954\n"
     ]
    }
   ],
   "source": [
    "# create a dictionary, an index for each word\n",
    "dictionary = dict()\n",
    "for i, word in enumerate(words):\n",
    "    dictionary[word] = i\n",
    "print(\"index of `hello`: %s\" % dictionary[\"hello\"])\n",
    "\n",
    "# a mapping for indexes back into words\n",
    "idx2word = {}\n",
    "for word, i in dictionary.items():\n",
    "    idx2word[i] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-23T10:55:05.788856Z",
     "start_time": "2018-03-23T10:55:05.722136Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence of word indexes for each sentence: [2238, 9282, 4107, 4046, 6600, 7949, 4079, 17655, 14533, 16476]\n"
     ]
    }
   ],
   "source": [
    "# convert sentences into sequences of word indexes\n",
    "sequences = [[dictionary[word] for word in sent] for sent in sents_lower]\n",
    "print(\"sequence of word indexes for each sentence: %s\" % sequences[0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-23T10:55:06.355669Z",
     "start_time": "2018-03-23T10:55:06.262758Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2238,  9282,  4107,  4046,  6600,  7949,  4079, 17655, 14533,\n",
       "       16476,  6267,  1646,  5989, 14612,  6025, 17220,  7450,  1410,\n",
       "         939, 17078,  1327, 13214,  8863, 14514,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pad sequences with zeros to make them same length: we need it for vectorized computations\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-23T10:55:07.064480Z",
     "start_time": "2018-03-23T10:55:06.996987Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# labels will be converted to categories: first indicates the probability of a capitalized word, second a lowercased word \n",
    "labels = pad_sequences(capitalization_sent_tags, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "labels = to_categorical(labels)\n",
    "labels[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-23T10:55:07.564914Z",
     "start_time": "2018-03-23T10:55:07.553220Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of test samples: 714\n"
     ]
    }
   ],
   "source": [
    "# our dataset will be split into a traing part and a validation part,\n",
    "# where we measure our model's performance during training,\n",
    "# this can be done automatically during the Keras model training\n",
    "\n",
    "# we will further keep a testing part to evaluate predictions \n",
    "TEST_SPLIT = .1\n",
    "nb_test_samples = int(TEST_SPLIT * data.shape[0])\n",
    "print(\"number of test samples: %s\" % nb_test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-23T10:55:08.153931Z",
     "start_time": "2018-03-23T10:55:08.136798Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we want to shuffle the data a bit to split the dataset uniformly\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "x_train = data[:-nb_test_samples]\n",
    "y_train = labels[:-nb_test_samples]\n",
    "x_test = data[-nb_test_samples:]\n",
    "y_test = labels[-nb_test_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-23T10:55:28.729901Z",
     "start_time": "2018-03-23T10:55:16.602656Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n",
      "embedding for the word `word`:\n",
      "[ 0.1233    0.55741   0.74203  -0.06547  -0.33485   0.81541  -0.16384\n",
      " -1.0327    0.41834  -0.012764 -0.60695   0.30146   0.35976   0.41161\n",
      "  0.03381  -0.091115  0.35077  -0.24798  -0.13128   0.19869   0.046961\n",
      "  0.014633 -0.39851  -0.11829  -0.27432  -0.032518 -0.23637  -0.072372\n",
      " -0.04237  -0.11159   0.12129   0.64011  -0.50275  -0.21584   0.30097\n",
      " -0.041772 -0.47972  -0.12897   0.6964   -0.27594  -0.29149   0.088033\n",
      "  0.12874  -0.15249  -0.20548   0.029435  0.055133 -0.12994  -0.33869\n",
      " -0.61891   0.4743    0.60288   1.0209    0.48663  -1.0587   -1.9711\n",
      " -0.41751   0.12457   1.304     0.26925   0.28003   0.91141  -0.62217\n",
      " -0.70356   1.0379   -0.095316  0.54085  -0.36123  -0.10311  -0.31059\n",
      " -0.61454   0.63799   0.18329  -0.49599   0.3607    0.70414  -0.28096\n",
      "  0.1062   -0.64866  -0.28698  -0.26623  -1.4502   -0.69456  -0.48722\n",
      " -1.6753    0.40353  -0.085219 -0.85528   0.65113   0.019457 -0.20924\n",
      "  0.18864  -0.12794   0.41757   0.097439 -0.58381  -0.38945  -0.15608\n",
      "  0.014198  0.65633 ]\n"
     ]
    }
   ],
   "source": [
    "# let's create a dictionary of embeddings from each word embedding vector in the pre-trained GloVe embeddings file\n",
    "GLOVE_DIR = \"./glove.6B/\"\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "print(\"embedding for the word `word`:\")\n",
    "print(embeddings_index.get(\"word\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-23T10:59:52.624713Z",
     "start_time": "2018-03-23T10:59:51.879450Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18482/18482 [00:00<00:00, 103529.04it/s]\n"
     ]
    }
   ],
   "source": [
    "# let's try to extract the GloVe embeddings for each word from our dataset vocabulary\n",
    "from tqdm import tqdm\n",
    "EMBEDDING_DIM = 100\n",
    "embedding_matrix = np.zeros((len(dictionary) + 1, EMBEDDING_DIM))\n",
    "for word, i in tqdm(dictionary.items()):\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "del embeddings_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking how many words have no pre-trained GloVe word embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-23T10:59:53.850605Z",
     "start_time": "2018-03-23T10:59:53.797467Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentage of words out of vocabulary: 6.709230602748621 percent\n",
      "examples of words without pre-trained GloVe embeddings:\n",
      "['', '37000', 'shoettle', 'esnards', 'future-day', 'hoijer', 'prosopopoeia', 'lagerlo', 'troubie', 'bullyboys', '300000', 'torrid-breeze', 'snobbishly', 'gaafer', 'schraffts']\n"
     ]
    }
   ],
   "source": [
    "oov_percentage = 100. * np.count_nonzero(np.all(embedding_matrix == 0, axis=1)) / len(dictionary)  # OOV portion\n",
    "print(\"percentage of words out of vocabulary: %s percent\" % oov_percentage)\n",
    "outta_vocab_idxs = set(np.where(np.all(embedding_matrix == 0, axis=1))[0])\n",
    "outta_vocab_words = [word for word, i in dictionary.items() if i in outta_vocab_idxs]\n",
    "print(\"examples of words without pre-trained GloVe embeddings:\")\n",
    "print(outta_vocab_words[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-23T10:59:55.772791Z",
     "start_time": "2018-03-23T10:59:55.754004Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_predictions(x_test, y_pred, idx2word):\n",
    "    \"\"\"\n",
    "    print and detokenize the results of our model's predictions\n",
    "    y_pred has two output channels, first one giving the probability of the lowercased word,\n",
    "    the second channel the probability of the capitalized word.\n",
    "    x_test are word indices of lowercased sentences\n",
    "    \"\"\"\n",
    "    for seq, preds in zip(x_test, y_pred):\n",
    "        sentence = []\n",
    "        for word_id, pred in zip(seq, preds):\n",
    "            if pred[0] > pred[1]:\n",
    "                sentence.append(idx2word[word_id])\n",
    "            else:\n",
    "                sentence.append(idx2word[word_id].capitalize())\n",
    "        print(MDETOK.detokenize(sentence, return_str=True).strip() + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple baseline: A single fully connected layer\n",
    "A Dense/fully connected layer won't remember the order of the words, but we might get some words right.\n",
    "\n",
    "Embeddings layer will be using the weights from the pre-trained GloVe vectors. We don't want to change them so we set `trainable=False`  \n",
    "\n",
    "We use softmax acivation, all outputs (2) of our model will sum up to one.\n",
    "\n",
    "20 % of the dataset will be used for validation.\n",
    "\n",
    "Number of epochs tells us how many times we want our model to look at all the traing samples in our traing set.\n",
    "\n",
    "If we set a number of epochs too high and our model is very complex, it will start overfitting the training data and the validation error will start rising."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-23T11:00:18.824579Z",
     "start_time": "2018-03-23T10:59:57.026479Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5144 samples, validate on 1286 samples\n",
      "Epoch 1/30\n",
      "5144/5144 [==============================] - 1s 158us/step - loss: 0.5960 - acc: 0.8087 - val_loss: 0.4881 - val_acc: 0.8836\n",
      "Epoch 2/30\n",
      "5144/5144 [==============================] - 1s 128us/step - loss: 0.4545 - acc: 0.8936 - val_loss: 0.4297 - val_acc: 0.8965\n",
      "Epoch 3/30\n",
      "5144/5144 [==============================] - 1s 152us/step - loss: 0.4062 - acc: 0.9023 - val_loss: 0.3896 - val_acc: 0.9030\n",
      "Epoch 4/30\n",
      "5144/5144 [==============================] - 1s 268us/step - loss: 0.3712 - acc: 0.9064 - val_loss: 0.3596 - val_acc: 0.9062\n",
      "Epoch 5/30\n",
      "5144/5144 [==============================] - 1s 223us/step - loss: 0.3451 - acc: 0.9094 - val_loss: 0.3372 - val_acc: 0.9079\n",
      "Epoch 6/30\n",
      "5144/5144 [==============================] - 1s 210us/step - loss: 0.3254 - acc: 0.9109 - val_loss: 0.3201 - val_acc: 0.9093\n",
      "Epoch 7/30\n",
      "5144/5144 [==============================] - 1s 206us/step - loss: 0.3104 - acc: 0.9132 - val_loss: 0.3073 - val_acc: 0.9141\n",
      "Epoch 8/30\n",
      "5144/5144 [==============================] - 1s 210us/step - loss: 0.2990 - acc: 0.9159 - val_loss: 0.2974 - val_acc: 0.9138\n",
      "Epoch 9/30\n",
      "5144/5144 [==============================] - 1s 218us/step - loss: 0.2902 - acc: 0.9162 - val_loss: 0.2898 - val_acc: 0.9138\n",
      "Epoch 10/30\n",
      "5144/5144 [==============================] - 1s 194us/step - loss: 0.2834 - acc: 0.9161 - val_loss: 0.2839 - val_acc: 0.9141\n",
      "Epoch 11/30\n",
      "5144/5144 [==============================] - 1s 137us/step - loss: 0.2781 - acc: 0.9162 - val_loss: 0.2793 - val_acc: 0.9144\n",
      "Epoch 12/30\n",
      "5144/5144 [==============================] - 1s 139us/step - loss: 0.2740 - acc: 0.9163 - val_loss: 0.2757 - val_acc: 0.9142\n",
      "Epoch 13/30\n",
      "5144/5144 [==============================] - 1s 143us/step - loss: 0.2707 - acc: 0.9163 - val_loss: 0.2729 - val_acc: 0.9145\n",
      "Epoch 14/30\n",
      "5144/5144 [==============================] - 1s 161us/step - loss: 0.2682 - acc: 0.9164 - val_loss: 0.2708 - val_acc: 0.9147\n",
      "Epoch 15/30\n",
      "5144/5144 [==============================] - 1s 127us/step - loss: 0.2662 - acc: 0.9167 - val_loss: 0.2691 - val_acc: 0.9146\n",
      "Epoch 16/30\n",
      "5144/5144 [==============================] - 1s 108us/step - loss: 0.2647 - acc: 0.9168 - val_loss: 0.2678 - val_acc: 0.9147\n",
      "Epoch 17/30\n",
      "5144/5144 [==============================] - 1s 105us/step - loss: 0.2634 - acc: 0.9169 - val_loss: 0.2667 - val_acc: 0.9150\n",
      "Epoch 18/30\n",
      "5144/5144 [==============================] - 0s 97us/step - loss: 0.2625 - acc: 0.9169 - val_loss: 0.2660 - val_acc: 0.9150\n",
      "Epoch 19/30\n",
      "5144/5144 [==============================] - 1s 101us/step - loss: 0.2617 - acc: 0.9169 - val_loss: 0.2653 - val_acc: 0.9147\n",
      "Epoch 20/30\n",
      "5144/5144 [==============================] - 1s 98us/step - loss: 0.2611 - acc: 0.9171 - val_loss: 0.2649 - val_acc: 0.9148\n",
      "Epoch 21/30\n",
      "5144/5144 [==============================] - 0s 94us/step - loss: 0.2607 - acc: 0.9169 - val_loss: 0.2645 - val_acc: 0.9153\n",
      "Epoch 22/30\n",
      "5144/5144 [==============================] - 0s 97us/step - loss: 0.2604 - acc: 0.9172 - val_loss: 0.2642 - val_acc: 0.9152\n",
      "Epoch 23/30\n",
      "5144/5144 [==============================] - 0s 93us/step - loss: 0.2601 - acc: 0.9167 - val_loss: 0.2640 - val_acc: 0.9151\n",
      "Epoch 24/30\n",
      "5144/5144 [==============================] - 0s 97us/step - loss: 0.2599 - acc: 0.9170 - val_loss: 0.2638 - val_acc: 0.9153\n",
      "Epoch 25/30\n",
      "5144/5144 [==============================] - 0s 96us/step - loss: 0.2597 - acc: 0.9169 - val_loss: 0.2639 - val_acc: 0.9150\n",
      "Epoch 26/30\n",
      "5144/5144 [==============================] - 1s 101us/step - loss: 0.2596 - acc: 0.9170 - val_loss: 0.2637 - val_acc: 0.9156\n",
      "Epoch 27/30\n",
      "5144/5144 [==============================] - 1s 99us/step - loss: 0.2595 - acc: 0.9170 - val_loss: 0.2638 - val_acc: 0.9148\n",
      "Epoch 28/30\n",
      "5144/5144 [==============================] - 0s 95us/step - loss: 0.2594 - acc: 0.9170 - val_loss: 0.2636 - val_acc: 0.9150\n",
      "Epoch 29/30\n",
      "5144/5144 [==============================] - 0s 94us/step - loss: 0.2594 - acc: 0.9170 - val_loss: 0.2637 - val_acc: 0.9154\n",
      "Epoch 30/30\n",
      "5144/5144 [==============================] - 0s 97us/step - loss: 0.2593 - acc: 0.9170 - val_loss: 0.2635 - val_acc: 0.9152\n",
      "714/714 [==============================] - 0s 50us/step\n",
      "Test accuracy: 0.9188075200850222\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(dictionary) + 1, output_dim=EMBEDDING_DIM, weights=[embedding_matrix],\n",
    "                    input_length=MAX_SEQUENCE_LENGTH, trainable=False))  # Note the embedding_matrix above gives the glove vecs\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, validation_split=0.2, epochs=30, batch_size=BATCH_SIZE)\n",
    "score, acc = model.evaluate(x_test, y_test, batch_size=BATCH_SIZE)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-23T11:03:53.100403Z",
     "start_time": "2018-03-23T11:03:52.988496Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he came by and repeated, Po 'Chavis!\n",
      "\n",
      "one of the proteases has ph optimum of about 3.7 and another of about 5.7 ( mcquillan, Stanley and trikojus, 1954; alpers, Robbins and rall, 1955).\n",
      "\n",
      "there, gregorio said, huff Wrote a complete statement of his offense.\n",
      "\n",
      "the season will open at the new Hall of flowers in Golden gate Park on november 20 at 8: 30 P. M. with a concert by the Mills Chamber players.\n",
      "\n",
      "thornburg added in a lower voice but Andy overheard, they act more like a jury than an audience.\n",
      "\n",
      "have you investigated the possibility of moving midweek holidays forward to monday or back to friday in order to have an uninterrupted work week?\n",
      "\n",
      "three were doubles, Brooks Robinson getting a pair and marv breeding one.\n",
      "\n",
      "a week later the daily journal had discovered the initial plans of some Providence citizens to hold a meeting honoring John Brown on the day of his execution.\n",
      "\n",
      "what Hume calls sensation is what Whitehead calls perception in the mode of presentational immediacy which is a sophisticated abstraction from perception in the mode of causal efficacy.\n",
      "\n",
      "Marty smiled at squire pleasantly and said, there was a cab waiting for me here.\n",
      "\n",
      "good heavens, adam, he said, I thought one thing you'd have no trouble learning is when to get out of a place.\n",
      "\n",
      "finally, Mercer did manage to follow b 'dikkat to the door of the cabin.\n",
      "\n",
      "I could scratch her eyes out, Eileen cried and stamped her foot when I came back from the phone booth.\n",
      "\n",
      "there's a walk there that goes out to quebec drive.\n",
      "\n",
      "the Manchester depot sewer company issued 214 shares of stock at $10 each for construction of a sewer in that locality, and assessments were made for its maintenance.\n",
      "\n",
      "customary Senate rules were ignored in order to speed approval of the Negro leader as administrator of the housing and home finance agency.\n",
      "\n",
      "we ran into a guy at the Pagan room who guarantees we can beat the wheel.\n",
      "\n",
      "his fellow countryman, igor stravinsky, certainly did not.\n",
      "\n",
      "in 1954 I was drafted and after serving two years honorably on active duty I was not required to participate in any further Army reserve activities.\n",
      "\n",
      "one of the inescapable realities of the cold War is that is has thrust upon the West a wholly new and historically unique set of moral dilemmas.\n",
      "\n",
      "the only original works attempting to reach any stature: Tennessee Williams' disappointing domestic comedy, period of adjustment, and Arthur laurents' clever but empty invitation to a march.\n",
      "\n",
      "I'm Mrs. Gertrude Parker, a soft voice explained, and I'd like to talk to you for a few minutes, please.\n",
      "\n",
      "over $200000000 is paid yearly to the 80000 full-time fortune-tellers in the United States by fearful mankind who want to know what the future holds!\n",
      "\n",
      "a few years ago a timex all Star Jazz show offered a broad range of styles, ranging from Lionel Hampton's big band to the free-wheeling Dukes of dixieland.\n",
      "\n",
      "while five minutes ago the place had presented a scene of easy revelry, with gyp carmer a prominent figure, it was now as somnolent and dull as the day before payday.\n",
      "\n",
      "split badly during the recent presidential election into almost equally divided camps of party loyalists and independents, the democratic party in Mississippi is currently a wreck.\n",
      "\n",
      "recently I Visited the very remarkable Pilgrim School for retarded children.\n",
      "\n",
      "in the same way I like to Think we owe our loyalty as legislators to our community, our District, our State.\n",
      "\n",
      "several at the King Arthur.\n",
      "\n",
      "this accounts for the wide variance in assessment practices of movable tangible property in the various municipalities in Rhode Island.\n",
      "\n",
      "four additional deputies be employed at the Fulton County jail and a doctor, medical intern or extern be employed for night and weekend duty at the jail.\n",
      "\n",
      "O 'banion drew his guns and fired at dave, severely wounding him in the stomach.\n",
      "\n",
      "discoveries recently made of old biblical manuscripts in Hebrew and Greek and other ancient writings, some by the early Church fathers, in themselves called for a restudy of the Bible.\n",
      "\n",
      "in laos, the administration looked at the Eisenhower administration efforts to show determination by sailing a Naval fleet into southeast asian waters as a useless gesture.\n",
      "\n",
      "the prime minister paid his respects to the buddhist monks, strode rapidly among the houses, joked with the local soldiery, and made a speech.\n",
      "\n",
      "I clamped a 30 - inch piece of aluminum to the base of the planer with a pair of sure grips.\n",
      "\n",
      "the commission seems to represent the viewpoint of what I would call the unconscious Liberal, but not unconscious enough, to invoke the now taboo symbolism of socialism.\n",
      "\n",
      "new, indeed, is Luther's perception, but not modern, as anyone knows who has ever tried to make intelligible to modern students what Luther was getting at.\n",
      "\n",
      "the deadlock has been caused by the russians' new demand for a three-man ( East, West and neutral) directorate, and thus a veto, over the control machinery.\n",
      "\n",
      "the removal of Stalin's body from the mausoleum he shared with Lenin to less distinguished quarters in the kremlin wall is not unprecedented in history.\n",
      "\n",
      "stardel ( Star's pride-starlette Hanover), 2: 34 h, looks quite promising.\n",
      "\n",
      "he is starred against Alvin Alcorn, trumpet; Phil Gomez, clarinet; cedric Haywood, piano; Julian Davidson, guitar; wellman braud, bass, and minor Hall, drums.\n",
      "\n",
      "since moving from a Chicago suburb to Southern California a few months ago, I've been introduced to a new game called lanesmanship.\n",
      "\n",
      "a man named lars Simon, playwright director, had expressed a wish that Anthony Payne drop dead.\n",
      "\n",
      "a nice bachelor apartment in a place called the Lancaster arms.\n",
      "\n",
      "there were thirty-eight patients on the bus the morning I left for Hanover, most of them disturbed and hallucinating.\n",
      "\n",
      "however, it is not enough to bring persons to Christian commitment and train them in the meaning of Christian discipleship.\n",
      "\n",
      "the advisory committee on administrative and budgetary questions is expected to receive the report this week.\n",
      "\n",
      "there is some evidence that naturally occurring goitrogens may play a role in the development of goitre, particularly in tasmania and australia ( clements and wishart, 1956).\n",
      "\n",
      "Jesus answered and said to him [ nicodemus] amen, amen, I say to Thee, unless a man be Born again, he cannot see the Kingdom of god.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let's see what our model predicts\n",
    "y_pred = model.predict(x_test)\n",
    "print_predictions(x_test[:50], y_pred[:50], idx2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Model\n",
    "add a Bidirectional layer of LSTM cells or GRU cells after the Embedding layer: \n",
    "\n",
    "`Bidirectional(cells, input_shape=(MAX_SEQUENCE_LENGTH, EMBEDDING_DIM))`\n",
    "\n",
    "you can try experimenting with different `cells` (LSTM, RNN,...):\n",
    "\n",
    "Try `LSTM(HIDDEN_SIZE_LSTM, return_sequences=...)` or `GRU(HIDDEN_SIZE_LSTM, return_sequences=...)`, can you see any difference in training or accuracy when using different cells?\n",
    "\n",
    "if we set `return_sequences` True, we will get the output of the cells in each timestep of the sequence, that's what we want:)\n",
    "\n",
    "you can change the complexity of the model by setting `HIDDEN_SIZE_LSTM` which changes the number of `units`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-23T11:20:53.098975Z",
     "start_time": "2018-03-23T11:18:45.277040Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5144 samples, validate on 1286 samples\n",
      "Epoch 1/10\n",
      "5144/5144 [==============================] - 13s 2ms/step - loss: 0.2119 - acc: 0.9153 - val_loss: 0.1386 - val_acc: 0.9473\n",
      "Epoch 2/10\n",
      "5144/5144 [==============================] - 12s 2ms/step - loss: 0.1162 - acc: 0.9574 - val_loss: 0.1104 - val_acc: 0.9593\n",
      "Epoch 3/10\n",
      "5144/5144 [==============================] - 12s 2ms/step - loss: 0.0968 - acc: 0.9643 - val_loss: 0.0972 - val_acc: 0.9643\n",
      "Epoch 4/10\n",
      "5144/5144 [==============================] - 12s 2ms/step - loss: 0.0845 - acc: 0.9686 - val_loss: 0.0901 - val_acc: 0.9674\n",
      "Epoch 5/10\n",
      "5144/5144 [==============================] - 12s 2ms/step - loss: 0.0770 - acc: 0.9715 - val_loss: 0.0845 - val_acc: 0.9695\n",
      "Epoch 6/10\n",
      "5144/5144 [==============================] - 12s 2ms/step - loss: 0.0712 - acc: 0.9735 - val_loss: 0.0807 - val_acc: 0.9719\n",
      "Epoch 7/10\n",
      "5144/5144 [==============================] - 13s 2ms/step - loss: 0.0653 - acc: 0.9762 - val_loss: 0.0775 - val_acc: 0.9723\n",
      "Epoch 8/10\n",
      "5144/5144 [==============================] - 12s 2ms/step - loss: 0.0604 - acc: 0.9775 - val_loss: 0.0774 - val_acc: 0.9733\n",
      "Epoch 9/10\n",
      "5144/5144 [==============================] - 14s 3ms/step - loss: 0.0571 - acc: 0.9790 - val_loss: 0.0751 - val_acc: 0.9728\n",
      "Epoch 10/10\n",
      "5144/5144 [==============================] - 13s 2ms/step - loss: 0.0523 - acc: 0.9810 - val_loss: 0.0753 - val_acc: 0.9736\n",
      "714/714 [==============================] - 0s 671us/step\n",
      "Test accuracy: 0.9729491859590974\n"
     ]
    }
   ],
   "source": [
    "HIDDEN_SIZE_LSTM = 100\n",
    "BATCH_SIZE = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(dictionary) + 1, output_dim=EMBEDDING_DIM, weights=[embedding_matrix],\n",
    "                    input_length=MAX_SEQUENCE_LENGTH, trainable=False))\n",
    "model.add(Bidirectional(LSTM(HIDDEN_SIZE_LSTM, return_sequences=True), input_shape=(MAX_SEQUENCE_LENGTH, EMBEDDING_DIM)))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, validation_split=0.2, epochs=10, batch_size=BATCH_SIZE)\n",
    "score, acc = model.evaluate(x_test, y_test, batch_size=BATCH_SIZE)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-23T11:20:54.155821Z",
     "start_time": "2018-03-23T11:20:53.532592Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He came by and repeated, Po 'Chavis!\n",
      "\n",
      "One of the proteases has ph optimum of about 3.7 and another of about 5.7 ( Mcquillan, Stanley and Trikojus, 1954; Alpers, Robbins and Rall, 1955).\n",
      "\n",
      "There, Gregorio said, Huff wrote a complete statement of his offense.\n",
      "\n",
      "The season will open at the new hall of flowers in Golden Gate Park on November 20 at 8: 30 p. m. with a concert by the Mills Chamber players.\n",
      "\n",
      "Thornburg added in a lower voice but Andy overheard, they act more like a jury than an audience.\n",
      "\n",
      "Have you investigated the possibility of moving midweek holidays forward to Monday or back to Friday in order to have an uninterrupted work week?\n",
      "\n",
      "Three were doubles, Brooks Robinson getting a pair and Marv breeding one.\n",
      "\n",
      "A week later the daily Journal had discovered the initial plans of some Providence citizens to hold a meeting honoring John Brown on the day of his execution.\n",
      "\n",
      "What Hume calls sensation is what Whitehead calls perception in the mode of presentational immediacy which is a sophisticated abstraction from perception in the mode of causal efficacy.\n",
      "\n",
      "Marty smiled at Squire pleasantly and said, there was a cab waiting for me here.\n",
      "\n",
      "Good heavens, Adam, he said, I thought one thing you'd have no trouble learning is when to get out of a place.\n",
      "\n",
      "Finally, Mercer did manage to follow B 'dikkat to the door of the cabin.\n",
      "\n",
      "I could scratch her eyes out, Eileen cried and stamped her foot when I came back from the phone Booth.\n",
      "\n",
      "There's a walk there that goes out to Quebec drive.\n",
      "\n",
      "The Manchester Depot Sewer Company issued 214 shares of stock at $10 each for construction of a sewer in that locality, and assessments were made for its maintenance.\n",
      "\n",
      "Customary Senate rules were ignored in order to speed approval of the Negro leader as administrator of the Housing and home Finance agency.\n",
      "\n",
      "We ran into a guy at the Pagan room who guarantees we can beat the wheel.\n",
      "\n",
      "His fellow Countryman, Igor Stravinsky, certainly did not.\n",
      "\n",
      "In 1954 I was drafted and after serving two years honorably on active duty I was not required to participate in any further Army reserve activities.\n",
      "\n",
      "One of the inescapable realities of the cold War is that is has thrust upon the West a wholly new and historically unique set of moral dilemmas.\n",
      "\n",
      "The only original works attempting to reach any stature: Tennessee Williams' disappointing domestic comedy, period of adjustment, and Arthur Laurents' clever but empty invitation to a March.\n",
      "\n",
      "I'm Mrs. Gertrude Parker, a soft voice explained, and I'd like to talk to you for a few minutes, please.\n",
      "\n",
      "Over $200000000 is paid yearly to the 80000 full-time fortune-tellers in the United States by fearful mankind who want to know what the future holds!\n",
      "\n",
      "A few years ago a Timex all Star Jazz show offered a broad range of styles, ranging from Lionel Hampton's big band to the free-wheeling Dukes of Dixieland.\n",
      "\n",
      "While five minutes ago the place had presented a scene of easy revelry, with Gyp Carmer a prominent figure, it was now as somnolent and dull as the day before payday.\n",
      "\n",
      "Split badly during the recent Presidential election into almost equally divided camps of party loyalists and independents, the Democratic party in Mississippi is currently a wreck.\n",
      "\n",
      "Recently I visited the very remarkable Pilgrim school for retarded children.\n",
      "\n",
      "In the same way I like to think we owe our loyalty as legislators to our community, our district, our state.\n",
      "\n",
      "Several at the King Arthur.\n",
      "\n",
      "This accounts for the wide variance in assessment practices of movable tangible property in the various municipalities in Rhode Island.\n",
      "\n",
      "Four additional deputies be employed at the Fulton County Jail and a doctor, medical intern or extern be employed for night and weekend duty at the jail.\n",
      "\n",
      "O 'Banion drew his guns and fired at Dave, severely wounding him in the stomach.\n",
      "\n",
      "Discoveries recently made of old Biblical manuscripts in Hebrew and Greek and other ancient writings, some by the early Church fathers, in themselves called for a restudy of the Bible.\n",
      "\n",
      "In Laos, the administration looked at the Eisenhower Administration efforts to show determination by sailing a naval fleet into Southeast Asian waters as a useless gesture.\n",
      "\n",
      "The prime minister paid his respects to the Buddhist monks, strode rapidly among the houses, joked with the local soldiery, and made a speech.\n",
      "\n",
      "I clamped a 30 - inch piece of aluminum to the base of the planer with a pair of sure grips.\n",
      "\n",
      "The commission seems to represent the viewpoint of what I would call the unconscious liberal, but not unconscious enough, to invoke the now taboo symbolism of Socialism.\n",
      "\n",
      "New, indeed, is Luther's perception, but not modern, as anyone knows who has ever tried to make intelligible to modern students what Luther was getting at.\n",
      "\n",
      "The deadlock has been caused by the Russians' new demand for a three-man ( East, West and neutral) Directorate, and thus a veto, over the control machinery.\n",
      "\n",
      "The removal of Stalin's body from the Mausoleum he shared with Lenin to less distinguished quarters in the kremlin wall is not unprecedented in history.\n",
      "\n",
      "Stardel ( Star's Pride-starlette Hanover), 2: 34 h, looks quite promising.\n",
      "\n",
      "He is starred against Alvin Alcorn, trumpet; Phil Gomez, clarinet; Cedric Haywood, piano; Julian Davidson, guitar; Wellman Braud, bass, and minor Hall, drums.\n",
      "\n",
      "Since moving from a Chicago suburb to Southern California a few months ago, I've been introduced to a new game called Lanesmanship.\n",
      "\n",
      "A man named Lars Simon, playwright director, had expressed a wish that Anthony Payne drop dead.\n",
      "\n",
      "A nice bachelor apartment in a place called the Lancaster Arms.\n",
      "\n",
      "There were thirty-eight patients on the bus the morning I left for Hanover, most of them disturbed and hallucinating.\n",
      "\n",
      "However, it is not enough to bring persons to Christian commitment and train them in the meaning of Christian Discipleship.\n",
      "\n",
      "The Advisory Committee on Administrative and budgetary questions is expected to receive the report this Week.\n",
      "\n",
      "There is some evidence that naturally occurring goitrogens may play a role in the development of Goitre, particularly in Tasmania and Australia ( Clements and Wishart, 1956).\n",
      "\n",
      "Jesus answered and said to him [ Nicodemus] amen, Amen, I say to thee, unless a man be born again, he cannot see the Kingdom of God.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let's see what our model predicts\n",
    "y_pred = model.predict(x_test)\n",
    "print_predictions(x_test[:50], y_pred[:50], idx2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Model\n",
    "\n",
    "Use Conv1d instead of RNN layers:\n",
    "\n",
    "`Conv1D(filters=EMBEDDING_DIM, kernel_size=..., activation='relu', padding=...)`\n",
    "\n",
    "We need to preserve the sequence legth, when going from one layer to another, so we set padding='same'\n",
    "\n",
    "kernel_size (window size) is a parameter setting the scope of view for our convolutional filter, how many words we look at.\n",
    "\n",
    "We want a filter for each index of our word embedding vector\n",
    "\n",
    "Try setting padding='causal'. This will make our window (kernel) wider, but we will look only at every other word in a sequence inside the window. This is also called a dilated convolution.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-23T11:25:23.688759Z",
     "start_time": "2018-03-23T11:25:02.073028Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5144 samples, validate on 1286 samples\n",
      "Epoch 1/10\n",
      "5144/5144 [==============================] - 2s 467us/step - loss: 0.2881 - acc: 0.9234 - val_loss: 0.1586 - val_acc: 0.9516\n",
      "Epoch 2/10\n",
      "5144/5144 [==============================] - 2s 383us/step - loss: 0.1219 - acc: 0.9589 - val_loss: 0.1134 - val_acc: 0.9602\n",
      "Epoch 3/10\n",
      "5144/5144 [==============================] - 2s 381us/step - loss: 0.0942 - acc: 0.9665 - val_loss: 0.1024 - val_acc: 0.9629\n",
      "Epoch 4/10\n",
      "5144/5144 [==============================] - 2s 416us/step - loss: 0.0814 - acc: 0.9708 - val_loss: 0.0967 - val_acc: 0.9655\n",
      "Epoch 5/10\n",
      "5144/5144 [==============================] - 2s 411us/step - loss: 0.0720 - acc: 0.9747 - val_loss: 0.0935 - val_acc: 0.9661\n",
      "Epoch 6/10\n",
      "5144/5144 [==============================] - 2s 391us/step - loss: 0.0641 - acc: 0.9772 - val_loss: 0.0911 - val_acc: 0.9673\n",
      "Epoch 7/10\n",
      "5144/5144 [==============================] - 2s 416us/step - loss: 0.0581 - acc: 0.9794 - val_loss: 0.0904 - val_acc: 0.9677\n",
      "Epoch 8/10\n",
      "5144/5144 [==============================] - 2s 444us/step - loss: 0.0525 - acc: 0.9815 - val_loss: 0.0904 - val_acc: 0.9679\n",
      "Epoch 9/10\n",
      "5144/5144 [==============================] - 2s 377us/step - loss: 0.0482 - acc: 0.9836 - val_loss: 0.0927 - val_acc: 0.9676\n",
      "Epoch 10/10\n",
      "5144/5144 [==============================] - 2s 365us/step - loss: 0.0440 - acc: 0.9851 - val_loss: 0.0920 - val_acc: 0.9683\n",
      "714/714 [==============================] - 0s 172us/step\n",
      "Test accuracy: 0.9690676318163297\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "WINDOW_SIZES = [3, 3]\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(dictionary) + 1, output_dim=EMBEDDING_DIM, weights=[embedding_matrix],\n",
    "                    input_length=MAX_SEQUENCE_LENGTH, trainable=False))\n",
    "# TODO: Try experimenting...adding some 1D Convolutinal layers here.\n",
    "model.add(Conv1D(filters=EMBEDDING_DIM, kernel_size=5, activation='relu', padding='same'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, validation_split=0.2, epochs=10, batch_size=BATCH_SIZE)\n",
    "score, acc = model.evaluate(x_test, y_test, batch_size=BATCH_SIZE)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-23T11:33:46.517507Z",
     "start_time": "2018-03-23T11:33:46.234651Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He came by and repeated, Po 'Chavis!\n",
      "\n",
      "One of the proteases has ph optimum of about 3.7 and another of about 5.7 ( Mcquillan, Stanley and Trikojus, 1954; Alpers, Robbins and Rall, 1955).\n",
      "\n",
      "There, Gregorio said, Huff wrote a complete statement of his offense.\n",
      "\n",
      "The season will open at the new hall of flowers in Golden Gate Park on November 20 at 8: 30 p. m. with a concert by the Mills Chamber players.\n",
      "\n",
      "Thornburg added in a lower voice but Andy overheard, they act more like a jury than an audience.\n",
      "\n",
      "Have you investigated the possibility of moving midweek holidays forward to Monday or back to Friday in order to have an uninterrupted work week?\n",
      "\n",
      "Three were doubles, Brooks Robinson getting a pair and Marv breeding one.\n",
      "\n",
      "A week later the daily Journal had discovered the initial plans of some Providence citizens to hold a meeting honoring John Brown on the day of his execution.\n",
      "\n",
      "What Hume calls sensation is what Whitehead calls perception in the mode of presentational immediacy which is a sophisticated abstraction from perception in the mode of causal efficacy.\n",
      "\n",
      "Marty smiled at Squire pleasantly and said, there was a cab waiting for me here.\n",
      "\n",
      "Good heavens, Adam, he said, I thought one thing you'd have no trouble learning is when to get out of a place.\n",
      "\n",
      "Finally, Mercer did manage to follow B 'dikkat to the door of the cabin.\n",
      "\n",
      "I could scratch her eyes out, Eileen cried and stamped her foot when I came back from the phone Booth.\n",
      "\n",
      "There's a walk there that goes out to Quebec drive.\n",
      "\n",
      "The Manchester Depot Sewer Company issued 214 shares of stock at $10 each for construction of a sewer in that locality, and assessments were made for its maintenance.\n",
      "\n",
      "Customary Senate rules were ignored in order to speed approval of the Negro leader as administrator of the housing and home Finance Agency.\n",
      "\n",
      "We ran into a guy at the Pagan room who guarantees we can beat the wheel.\n",
      "\n",
      "His fellow Countryman, Igor Stravinsky, certainly did not.\n",
      "\n",
      "In 1954 I was drafted and after serving two years honorably on active duty I was not required to participate in any further army reserve activities.\n",
      "\n",
      "One of the inescapable realities of the cold war is that is has thrust upon the West a wholly new and historically unique set of moral dilemmas.\n",
      "\n",
      "The only original works attempting to reach any stature: Tennessee Williams' disappointing domestic comedy, period of adjustment, and Arthur Laurents' clever but empty invitation to a March.\n",
      "\n",
      "I'm Mrs. Gertrude Parker, a soft voice explained, and I'd like to talk to you for a few minutes, please.\n",
      "\n",
      "Over $200000000 is paid yearly to the 80000 full-time fortune-tellers in the United States by fearful mankind who want to know what the future Holds!\n",
      "\n",
      "A few years ago a timex all Star Jazz show offered a broad range of styles, ranging from Lionel Hampton's big band to the free-wheeling dukes of Dixieland.\n",
      "\n",
      "While five minutes ago the place had presented a scene of easy revelry, with Gyp Carmer A prominent figure, it was now as somnolent and dull as the day before payday.\n",
      "\n",
      "split badly during the recent presidential election into almost equally divided camps of party loyalists and independents, the Democratic party in Mississippi is currently a wreck.\n",
      "\n",
      "Recently I visited the very remarkable Pilgrim School for retarded children.\n",
      "\n",
      "In the same way I like to think we owe our loyalty as legislators to our community, our District, our state.\n",
      "\n",
      "Several at the King Arthur.\n",
      "\n",
      "This accounts for the wide variance in assessment practices of movable tangible property in the various municipalities in Rhode Island.\n",
      "\n",
      "Four additional deputies be employed at the Fulton County jail and a doctor, Medical intern or extern be employed for night and weekend duty at the jail.\n",
      "\n",
      "O 'Banion drew his guns and fired at Dave, severely wounding him in the stomach.\n",
      "\n",
      "Discoveries recently made of Old Biblical manuscripts in Hebrew and Greek and other ancient writings, some by the early Church fathers, in themselves called for a restudy of the Bible.\n",
      "\n",
      "in Laos, the Administration looked at the Eisenhower administration efforts to show determination by sailing a Naval fleet into Southeast Asian waters as a useless gesture.\n",
      "\n",
      "The prime minister paid his respects to the Buddhist monks, strode rapidly among the houses, joked with the local soldiery, and made a speech.\n",
      "\n",
      "I clamped a 30 - inch piece of aluminum to the base of the planer with a pair of sure grips.\n",
      "\n",
      "The commission seems to represent the viewpoint of what I would call the unconscious liberal, but not unconscious enough, to invoke the now taboo symbolism of Socialism.\n",
      "\n",
      "New, indeed, is Luther's perception, but not modern, as anyone knows who has ever tried to make intelligible to modern students what Luther was getting at.\n",
      "\n",
      "The deadlock has been caused by the Russians' new demand for a three-man ( East, west and neutral) directorate, and thus a veto, over the control machinery.\n",
      "\n",
      "The removal of Stalin's body from the mausoleum he shared with Lenin to less distinguished quarters in the kremlin wall is not unprecedented in history.\n",
      "\n",
      "Stardel ( Star's pride-starlette Hanover), 2: 34 h, looks quite promising.\n",
      "\n",
      "He is starred against Alvin Alcorn, trumpet; Phil Gomez, Clarinet; Cedric Haywood, piano; Julian Davidson, guitar; Wellman Braud, bass, and minor hall, drums.\n",
      "\n",
      "Since moving from a Chicago Suburb to Southern California a few months ago, I've been introduced to a new game called Lanesmanship.\n",
      "\n",
      "A man named Lars Simon, playwright director, had expressed a wish that Anthony Payne drop dead.\n",
      "\n",
      "A nice bachelor apartment in a place called the Lancaster Arms.\n",
      "\n",
      "There were thirty-eight patients on the bus the morning I left for Hanover, most of them disturbed and hallucinating.\n",
      "\n",
      "However, it is not enough to bring persons to Christian commitment and train them in the meaning of Christian discipleship.\n",
      "\n",
      "The advisory committee on administrative and budgetary questions is expected to receive the report this week.\n",
      "\n",
      "There is some evidence that naturally occurring goitrogens may play a role in the development of Goitre, particularly in Tasmania and Australia ( Clements and Wishart, 1956).\n",
      "\n",
      "Jesus answered and said to him [ Nicodemus] amen, amen, I say to thee, unless a man be born again, he cannot see the Kingdom of God.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let's see what our model predicts\n",
    "y_pred = model.predict(x_test)\n",
    "print_predictions(x_test[:50], y_pred[:50], idx2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Character Level Models\n",
    "\n",
    "We are now going to work with models that look at each letter of the text, deciding whether it should be big or small.\n",
    "The strategy can be the same as with the words, but now we have no pre-trained vector embeddings, so we initialize them randomly and allow their training by setting trainable=True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-23T11:33:48.878526Z",
     "start_time": "2018-03-23T11:33:48.873453Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HIDDEN_SIZE_LSTM = 100\n",
    "EMBEDDING_DIM = 20\n",
    "BATCH_SIZE = 32\n",
    "WIN_SIZE = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-23T11:35:14.044682Z",
     "start_time": "2018-03-23T11:35:10.231793Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size: 59\n"
     ]
    }
   ],
   "source": [
    "# our dataset will be split into a traing part and a validation part, where we measure our model's performance\n",
    "VALIDATION_SPLIT = .2\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * len(sents))\n",
    "indices = np.arange(len(sents))\n",
    "np.random.shuffle(indices)\n",
    "train_indices = indices[:-nb_validation_samples]\n",
    "val_indices = indices[-nb_validation_samples:]\n",
    "\n",
    "\n",
    "whole_sents_train = [MDETOK.detokenize(sents[index], return_str=True) for index in train_indices]\n",
    "whole_sents_val = [MDETOK.detokenize(sents[index], return_str=True) for index in val_indices]\n",
    "whole_sents_lower_train = [sent.lower() for sent in whole_sents_train]\n",
    "whole_sents_lower_val = [sent.lower() for sent in whole_sents_val]\n",
    "\n",
    "whole_text_train = \" \".join(whole_sents_train)\n",
    "whole_text_val = \" \".join(whole_sents_val)\n",
    "whole_text_lower_train = whole_text_train.lower()\n",
    "whole_text_lower_val = whole_text_val.lower()\n",
    "\n",
    "# create a vocabulary\n",
    "chars_vocab = set(whole_text_lower_train).union(whole_text_lower_val)\n",
    "print(\"vocabulary size: %s\" % len(chars_vocab))\n",
    "\n",
    "# create a dictionary, an index for each character\n",
    "chars_dictionary = dict()\n",
    "for i, char in enumerate(chars_vocab):\n",
    "    chars_dictionary[char] = i\n",
    "\n",
    "# a mapping for indexes back into chars\n",
    "idx2char = {}\n",
    "for char, i in chars_dictionary.items():\n",
    "    idx2char[i] = char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at the whole sentence characters in a single prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-23T11:35:14.718095Z",
     "start_time": "2018-03-23T11:35:14.491647Z"
    }
   },
   "outputs": [],
   "source": [
    "capitalization_char_tags_train = [[char.isupper() for char in sent] for sent in whole_sents_train]\n",
    "capitalization_char_tags_val = [[char.isupper() for char in sent] for sent in whole_sents_val]\n",
    "\n",
    "# convert sentences into sequences of character indexes\n",
    "sequences_train = [[chars_dictionary[char] for char in sent] for sent in whole_sents_lower_train]\n",
    "sequences_val = [[chars_dictionary[char] for char in sent] for sent in whole_sents_lower_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-23T11:35:16.242778Z",
     "start_time": "2018-03-23T11:35:16.044435Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 200\n",
    "\n",
    "data_train = pad_sequences(sequences_train, maxlen=MAX_SEQUENCE_LENGTH, padding='post', value=chars_dictionary[\" \"])\n",
    "data_val = pad_sequences(sequences_val, maxlen=MAX_SEQUENCE_LENGTH, padding='post', value=chars_dictionary[\" \"])\n",
    "\n",
    "labels_train = pad_sequences(capitalization_char_tags_train, maxlen=MAX_SEQUENCE_LENGTH, padding='post', value=chars_dictionary[\" \"])[:,:,np.newaxis]\n",
    "labels_val = pad_sequences(capitalization_char_tags_val, maxlen=MAX_SEQUENCE_LENGTH, padding='post', value=chars_dictionary[\" \"])[:,:,np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-03-23T11:35:17.883Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5716 samples, validate on 1428 samples\n",
      "Epoch 1/10\n",
      "5716/5716 [==============================] - 71s 12ms/step - loss: -233.5051 - acc: 0.0245 - val_loss: -263.7156 - val_acc: 0.0234\n",
      "Epoch 2/10\n",
      "5716/5716 [==============================] - 65s 11ms/step - loss: -261.7936 - acc: 0.0232 - val_loss: -266.1873 - val_acc: 0.0234\n",
      "Epoch 3/10\n",
      "5716/5716 [==============================] - 65s 11ms/step - loss: -264.6626 - acc: 0.0231 - val_loss: -269.2184 - val_acc: 0.0234\n",
      "Epoch 4/10\n",
      "5716/5716 [==============================] - 69s 12ms/step - loss: -268.7038 - acc: 0.2297 - val_loss: -272.6378 - val_acc: 0.5054\n",
      "Epoch 5/10\n",
      "5716/5716 [==============================] - 79s 14ms/step - loss: -269.8985 - acc: 0.5189 - val_loss: -272.7449 - val_acc: 0.5204\n",
      "Epoch 6/10\n",
      "1952/5716 [=========>....................] - ETA: 53s - loss: -263.9205 - acc: 0.5349"
     ]
    }
   ],
   "source": [
    "HIDDEN_SIZE_LSTM = 100\n",
    "BATCH_SIZE = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(chars_dictionary) + 1, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, trainable=True))\n",
    "model.add(Bidirectional(LSTM(HIDDEN_SIZE_LSTM, return_sequences=True), input_shape=(MAX_SEQUENCE_LENGTH, EMBEDDING_DIM)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(data_train, labels_train, validation_data=(data_val, labels_val), epochs=10, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN model\n",
    "let's try to build a similar model like the one we used for words above, only keeping allowing the Embeddings to be trainable this time :)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO implement a character level CNN model, notice that now we output a single value as our prediction\n",
    "# before our output was categorical, one output for lowercase, one for uppercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_char_predictions(x_test, y_pred, idx2char):\n",
    "    \"\"\"\n",
    "    print the results of our model's predictions\n",
    "    y_pred is a number between zero and one, a probability of the letter being Uppercase.\n",
    "    x_test are char indices of lowercased sentences\n",
    "    \"\"\"\n",
    "    for seq, preds in zip(x_test, y_pred):\n",
    "        sentence = []\n",
    "        for char_id, pred in zip(seq, preds):\n",
    "            if pred > .5:\n",
    "                sentence.append(idx2char[char_id].upper())\n",
    "            else:\n",
    "                sentence.append(idx2char[char_id])\n",
    "        print(\"\".join(sentence) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(data_val)\n",
    "print_char_predictions(data_val, y_pred, idx2char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a smaller sliding window on a continuous text\n",
    "up to now, the sequences of characters were quite long (hundreds of characters)...We can have a look at continuous text and create a window, that will limit our scope to say 30 characters. We are going to slide the window one character forwards and make predictions in each of them.\n",
    "\n",
    "What to do with the excess predictions from overlapping windows?\n",
    "\n",
    "Let's just take the prediction from the middle of the window. So that we cover enough context from both sides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capitalization_char_tags_train = [char.isupper() for char in whole_text_train]\n",
    "capitalization_char_tags_val = [char.isupper() for char in whole_text_val]\n",
    "\n",
    "# convert sentences into sequences of character indexes\n",
    "sequence_train = [chars_dictionary[char] for char in whole_text_lower_train]\n",
    "sequence_val = [chars_dictionary[char] for char in whole_text_lower_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_batch_generator(sequence, labels, win_size=30, batch_size=32):\n",
    "    \"\"\"yield batches of sequences and lables in windows, each window sliding one step-ahead\"\"\"\n",
    "    x_batch, y_batch = [], []\n",
    "    while True:\n",
    "        for i in range(0, len(sequence) - win_size):\n",
    "            if len(x_batch) == batch_size:\n",
    "                yield np.array(x_batch), np.array(y_batch, dtype=\"float64\")[:,:,np.newaxis]\n",
    "                x_batch, y_batch = [], []\n",
    "            x_batch.append(sequence[i:i + win_size])\n",
    "            y_batch.append(labels[i:i + win_size])\n",
    "        if len(x_batch) != 0:\n",
    "            yield np.array(x_batch), np.array(y_batch, dtype=\"float64\")[:,:,np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_gen_train = window_batch_generator(sequence_train, capitalization_char_tags_train)\n",
    "data_gen_val = window_batch_generator(sequence_val, capitalization_char_tags_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will now provide our Keras model with a Generator that is going to collect samples batch by batch\n",
    "\n",
    "the generator is \"bottom-less\" repeating it's iteration after we go through the whole text (EPOCH)\n",
    "we will calculate the epoch size to let our model know when to go to next epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch_train = (len(sequence_train) - WIN_SIZE) / BATCH_SIZE\n",
    "steps_per_epoch_val = (len(sequence_val) - WIN_SIZE) / BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE_GRU = 10\n",
    "EMBEDDING_DIM = 20\n",
    "BATCH_SIZE = 32\n",
    "WIN_SIZE = 30\n",
    "NB_EPOCHS = 1\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(chars_dictionary) + 1, output_dim=EMBEDDING_DIM, input_length=WIN_SIZE, trainable=True))\n",
    "model.add(Bidirectional(GRU(HIDDEN_SIZE_GRU, return_sequences=True), input_shape=(WIN_SIZE, EMBEDDING_DIM)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit_generator(data_gen_train, validation_data=data_gen_val,\n",
    "                    epochs=NB_EPOCHS, steps_per_epoch=steps_per_epoch_train, validation_steps=steps_per_epoch_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "*We can take each overlapping window prediction and extract only its middle part to make sure we cover the neighboring characters from both sides:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_mid_window_predictions(batch_samples, model, idx2char):\n",
    "    \"\"\"\n",
    "    print the results of our model's predictions\n",
    "    take only the prediction from the middle of a window.\n",
    "    \"\"\"\n",
    "    win_size = test_samples[0].shape[1]\n",
    "    mid_win_idx = win_size / 2\n",
    "    extacted_text = []\n",
    "    for batch in test_samples:\n",
    "        predictions = model.predict_on_batch(batch)\n",
    "        for chars, preds in zip(batch, predictions):\n",
    "            if preds[mid_win_idx] > .5:\n",
    "                extacted_text.append(idx2char[chars[mid_win_idx]].upper())\n",
    "            else:\n",
    "                extacted_text.append(idx2char[chars[mid_win_idx]])\n",
    "    print(\"\".join(extacted_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "steps = 50\n",
    "test_samples = [data_gen_val.next()[0] for _ in range(steps)]  # we extract some test samples from our validation set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_mid_window_predictions(test_samples, model, idx2char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Predicting the first letter in a window only by learning to read backwards :D...we won't be able to recognize the beginnings of sentences, but perheaps we might succeed at catching some names, lets give it a try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def window_batch_generator_first_letter_out(sequence, labels, win_size=30, batch_size=32):\n",
    "    \"\"\"this generator only outputs a single label for each window, a label of our first character\"\"\"\n",
    "    x_batch, y_batch = [], []\n",
    "    while True:\n",
    "        for i in range(0, len(sequence) - win_size):\n",
    "            if len(x_batch) == batch_size:\n",
    "                yield np.array(x_batch), np.array(y_batch, dtype=\"float64\")[:, np.newaxis]\n",
    "                x_batch, y_batch = [], []\n",
    "            x_batch.append(sequence[i:i + win_size])\n",
    "            y_batch.append(labels[i])\n",
    "        if len(x_batch) != 0:\n",
    "            yield np.array(x_batch), np.array(y_batch, dtype=\"float64\")[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gen_train = window_batch_generator_first_letter_out(sequence_train, capitalization_char_tags_train)\n",
    "data_gen_val = window_batch_generator_first_letter_out(sequence_val, capitalization_char_tags_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are only making a prediction from the final RNN cell's output (last timestep) by setting return_sequences=False.\n",
    "\n",
    "Setting go_backwards=True is going to revert the order of the sequence when passing it to RNN.\n",
    "\n",
    "When training, we are giving only a single example (label) for the whole sequence of characters. This is similar to the task of IMDB movie sentiment. In the models before, we had labels for each letter in the sequence. Could it more difficult to train the model now as it sees less information?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE_GRU = 10\n",
    "EMBEDDING_DIM = 20\n",
    "BATCH_SIZE = 32\n",
    "WIN_SIZE = 30\n",
    "NB_EPOCHS = 3\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(chars_dictionary) + 1, output_dim=EMBEDDING_DIM, input_length=WIN_SIZE, trainable=True))\n",
    "model.add(GRU(HIDDEN_SIZE_GRU, return_sequences=False, go_backwards=True, input_shape=(WIN_SIZE, EMBEDDING_DIM)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit_generator(data_gen_train, validation_data=data_gen_val,\n",
    "                    epochs=NB_EPOCHS, steps_per_epoch=steps_per_epoch_train, validation_steps=steps_per_epoch_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_first_letter_predictions(batch_samples, model, idx2char):\n",
    "    \"\"\"\n",
    "    print the results of our model's predictions\n",
    "    take only the first letter prediction\n",
    "    \"\"\"\n",
    "    extacted_text = []\n",
    "    for batch in test_samples:\n",
    "        predictions = model.predict_on_batch(batch)\n",
    "        for chars, pred in zip(batch, predictions):\n",
    "            if pred > .5:\n",
    "                extacted_text.append(idx2char[chars[0]].upper())\n",
    "            else:\n",
    "                extacted_text.append(idx2char[chars[0]])\n",
    "    print(\"\".join(extacted_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "steps = 50\n",
    "test_samples = [data_gen_val.next()[0] for _ in range(steps)]  # we extract some test samples from our validation set\n",
    "print_first_letter_predictions(test_samples, model, idx2char)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "207px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
